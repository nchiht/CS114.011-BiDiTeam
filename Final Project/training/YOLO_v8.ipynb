{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/CS114/Pipeline/Material/kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d chithinguyen/vietnamese-book-covers\n",
        "!unzip /content/vietnamese-book-covers.zip"
      ],
      "metadata": {
        "id": "enqxxzaLDaxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/YOLO_data/train_images/images\n",
        "!mkdir /content/YOLO_data/train_images/labels\n",
        "%cd /content/YOLO_data/train_images/\n",
        "!mv *.txt labels\n",
        "!mv *.jpg images\n",
        "!mv *.JPG images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEzuOgXPwQiJ",
        "outputId": "a0090ee9-faeb-4a77-c498-4aae401358ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/YOLO_data/train_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/YOLO_data/test_images/images\n",
        "!mkdir /content/YOLO_data/test_images/labels\n",
        "%cd /content/YOLO_data/test_images/\n",
        "!mv *.txt labels\n",
        "!mv *.jpg images\n",
        "!mv *.JPG images"
      ],
      "metadata": {
        "id": "grpTYX67rPqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/YOLO_data/val_images/images\n",
        "!mkdir /content/YOLO_data/val_images/labels\n",
        "%cd /content/YOLO_data/val_images/\n",
        "!mv *.txt labels\n",
        "!mv *.jpg images\n",
        "!mv *.JPG images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuOJXXyb1EzY",
        "outputId": "240009d3-859c-484f-e134-e89831f61442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/YOLO_data/val_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/YOLO_data/\n",
        "!rm data.yaml\n",
        "!echo \"train: /content/YOLO_data/train_images\" >> data.yaml\n",
        "!echo \"val: /content/YOLO_data/val_images\" >> data.yaml\n",
        "!echo \"nc: 4\" >> data.yaml\n",
        "!echo \"names: ['other', 'author', 'title', 'publisher']\" >> data.yaml\n",
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvNawciE2Taj",
        "outputId": "685d365b-8f22-4eb5-a24d-2260d18f9951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/YOLO_data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YoloV8"
      ],
      "metadata": {
        "id": "EHPjHN3AQl_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "N_FhiTOsQwYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=/content/runs/detect/train3/weights/last.pt data=/content/Dataset/data.yaml epochs=10 imgsz=608 val=True"
      ],
      "metadata": {
        "id": "PZgXXBrwgb40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=predict model=/content/runs/detect/train/weights/best.pt source=/content/Dataset/YOLO_data/test_images"
      ],
      "metadata": {
        "id": "qkFJnsoQSQL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Test the model\n",
        "results = model.val()"
      ],
      "metadata": {
        "id": "oWbWCqdfbiGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "vcEzIH80UOlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YoloV5"
      ],
      "metadata": {
        "id": "X5AZ25MfQrxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt  # install"
      ],
      "metadata": {
        "id": "X34HctUzadSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/yolov5/train.py  --data /content/YOLO_data/data.yaml --weights yolov5n.pt --epochs 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmXO-ee6a-EM",
        "outputId": "db6ce4f3-8ff6-4c08-9cc0-ea52d018711b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-23 10:54:03.399775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 10:54:03.399828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 10:54:03.401293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n.pt, cfg=, data=/content/YOLO_data/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-278-g050c72c Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 17.0MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
            "100% 3.87M/3.87M [00:00<00:00, 58.3MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1     12177  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "Model summary: 214 layers, 1769329 parameters, 1769329 gradients, 4.2 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5n.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/YOLO_data/train_images/labels... 700 images, 0 backgrounds, 0 corrupt: 100% 700/700 [00:06<00:00, 112.78it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123401.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123409.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123440.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123615.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123636.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123734.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123824.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123832.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_123847.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_124055.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_124550.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_124603.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_124807.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/20231229_131137.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/2023_12_29_15_53_IMG_6036.JPG: 15 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/VH_012.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/VH_013.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/YOLO_data/train_images/images/VH_083.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/YOLO_data/train_images/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/YOLO_data/val_images/labels... 132 images, 0 backgrounds, 0 corrupt: 100% 132/132 [00:00<00:00, 143.26it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/YOLO_data/val_images/images/20231229_124645.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/YOLO_data/val_images/labels.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.40 anchors/target, 0.915 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ⚠️ Extremely small objects found: 82 of 7644 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 7644 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7219: 100% 1000/1000 [00:10<00:00, 91.63it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9791 best possible recall, 5.43 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.353/0.725-mean/best, past_thr=0.482-mean: 27,8, 49,13, 95,11, 102,21, 184,14, 83,50, 233,30, 205,54, 294,77\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9      1.95G     0.1226    0.08312    0.04719        183        640: 100% 44/44 [05:05<00:00,  6.95s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/5 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:10<00:00,  2.13s/it]\n",
            "                   all        132       1442     0.0284     0.0544     0.0161    0.00388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9      2.12G    0.09868    0.08847    0.03571        229        640: 100% 44/44 [05:13<00:00,  7.13s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:05<00:00,  1.15s/it]\n",
            "                   all        132       1442     0.0667      0.309     0.0699     0.0191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9      2.12G    0.08691    0.08455    0.03162        199        640: 100% 44/44 [05:22<00:00,  7.33s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:04<00:00,  1.13it/s]\n",
            "                   all        132       1442      0.105      0.317      0.127     0.0365\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9      2.12G    0.07861    0.08349    0.02989        287        640: 100% 44/44 [05:13<00:00,  7.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:04<00:00,  1.06it/s]\n",
            "                   all        132       1442      0.162      0.321      0.232     0.0839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9      2.12G    0.07538    0.07972    0.02824        221        640: 100% 44/44 [05:09<00:00,  7.04s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:03<00:00,  1.29it/s]\n",
            "                   all        132       1442      0.203      0.472      0.263      0.097\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9      2.12G    0.06977     0.0785    0.02816        215        640: 100% 44/44 [05:14<00:00,  7.14s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:03<00:00,  1.38it/s]\n",
            "                   all        132       1442      0.321      0.457      0.324       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9      2.12G     0.0654    0.07778    0.02778        181        640: 100% 44/44 [05:08<00:00,  7.01s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:03<00:00,  1.46it/s]\n",
            "                   all        132       1442      0.333      0.474      0.342      0.126\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9      2.12G    0.06314    0.07781    0.02707        202        640: 100% 44/44 [05:10<00:00,  7.07s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:05<00:00,  1.04s/it]\n",
            "                   all        132       1442      0.402      0.507      0.391      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9      2.12G    0.06006    0.07586     0.0267        249        640: 100% 44/44 [05:12<00:00,  7.09s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:03<00:00,  1.38it/s]\n",
            "                   all        132       1442      0.361      0.553      0.418      0.188\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9      2.12G    0.05781    0.07606    0.02609        182        640: 100% 44/44 [05:09<00:00,  7.03s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:03<00:00,  1.31it/s]\n",
            "                   all        132       1442      0.416      0.545      0.439      0.202\n",
            "\n",
            "10 epochs completed in 0.886 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 3.8MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 3.8MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1764577 parameters, 0 gradients, 4.1 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 5/5 [00:40<00:00,  8.03s/it]\n",
            "                   all        132       1442      0.417      0.543      0.439      0.202\n",
            "                 other        132        564      0.346      0.525       0.34      0.127\n",
            "                author        132        232      0.293      0.569      0.279      0.117\n",
            "                 title        132        469       0.53      0.904      0.815       0.45\n",
            "             publisher        132        177      0.499      0.175      0.319      0.114\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/yolov5/detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --source /content/Dataset/YOLO_data/val_images"
      ],
      "metadata": {
        "id": "NgsvMaBvuhS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}